#!/usr/bin/env node

/**
 * ØªØµØ¯ÙŠØ± ÙƒØ§Ù…Ù„ Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Supabase
 * ÙŠÙ‚ÙˆÙ… Ø¨ØªØµØ¯ÙŠØ± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù…Ù„ÙØ§Øª SQL
 */

import { createClient } from '@supabase/supabase-js';
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
import dotenv from 'dotenv';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Load environment variables
dotenv.config({ path: path.join(__dirname, '..', '.env') });

const supabaseUrl = process.env.VITE_SUPABASE_URL;
const supabaseKey = process.env.VITE_SUPABASE_ANON_KEY;

if (!supabaseUrl || !supabaseKey) {
  console.error('âŒ Error: Missing Supabase credentials in .env file');
  process.exit(1);
}

const supabase = createClient(supabaseUrl, supabaseKey);

// Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… ØªØµØ¯ÙŠØ± Ø¨ÙŠØ§Ù†Ø§ØªÙ‡Ø§
const DATA_TABLES = [
  // Core tables
  'roles',
  'departments',
  'regions',
  'cities',
  'districts',
  'old_regions',

  // Staff
  'staff',
  'staff_services',
  'staff_regions',

  // Services
  'services',
  'service_types',
  'service_fields',
  'service_documents',
  'service_requirements',
  'service_field_conditions',
  'service_document_conditions',
  'service_dynamic_list_fields',
  'service_pricing_rules',

  // Applications
  'applications',
  'application_notes',
  'application_statuses',
  'status_history',
  'otp_verifications',
  'payments',
  'rejection_details',

  // Application Pricing
  'application_pricing_items',
  'application_pricing_summary',
  'invoices',

  // Appointments & Shipping
  'appointment_settings',
  'appointment_slots',
  'appointments',
  'closed_days',
  'shipping_companies',
  'shipments',

  // Educational Cards
  'educational_cards',

  // CMS
  'site_settings',
  'contact_info',
  'social_links',
  'slider_items',
  'page_sections',
  'footer_content',
  'counters',

  // News & Events
  'breaking_news_ticker',
  'news',
  'events',
  'event_registrations',

  // About Pages
  'about_sudan_page',
  'about_sudan_statistics',
  'about_sudan_sections',
  'about_sudan_section_stats',
  'about_consulate_sections',
  'ambassadors',
  'services_guide_sections',
  'important_links',
  'additional_pages',

  // System
  'system_maintenance',
  'system_announcements',
  'system_settings',

  // Contact
  'contact_messages',

  // Chatbot
  'chatbot_categories',
  'chatbot_questions_answers',
  'chatbot_conversations',

  // Chat
  'chat_conversations',
  'chat_messages',
  'chat_staff',

  // Export
  'export_report_templates'
];

/**
 * ØªØ­ÙˆÙŠÙ„ Ù‚ÙŠÙ…Ø© JavaScript Ø¥Ù„Ù‰ SQL
 */
function toSQLValue(value) {
  if (value === null || value === undefined) {
    return 'NULL';
  }

  if (typeof value === 'boolean') {
    return value ? 'true' : 'false';
  }

  if (typeof value === 'number') {
    return value.toString();
  }

  if (typeof value === 'object') {
    // Handle arrays and JSON objects
    return `'${JSON.stringify(value).replace(/'/g, "''")}'::jsonb`;
  }

  if (typeof value === 'string') {
    // Escape single quotes
    return `'${value.replace(/'/g, "''")}'`;
  }

  return `'${value}'`;
}

/**
 * ØªØµØ¯ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙˆÙ„ Ø¥Ù„Ù‰ SQL INSERT statements
 */
async function exportTableData(tableName) {
  try {
    console.log(`ğŸ“¥ Exporting data from ${tableName}...`);

    const { data, error } = await supabase
      .from(tableName)
      .select('*')
      .order('created_at', { ascending: true, nullsFirst: false })
      .limit(10000);

    if (error) {
      console.error(`   âš ï¸  Warning: Could not export ${tableName}:`, error.message);
      return '';
    }

    if (!data || data.length === 0) {
      console.log(`   â„¹ï¸  No data in ${tableName}`);
      return `-- No data in ${tableName}\n\n`;
    }

    console.log(`   âœ“ Found ${data.length} rows`);

    let sql = `-- Data for table: ${tableName}\n`;
    sql += `-- Rows: ${data.length}\n\n`;

    // Get column names from first row
    const columns = Object.keys(data[0]);

    // Create INSERT statements in batches
    const batchSize = 100;
    for (let i = 0; i < data.length; i += batchSize) {
      const batch = data.slice(i, i + batchSize);

      sql += `INSERT INTO public.${tableName} (${columns.join(', ')})\nVALUES\n`;

      const values = batch.map(row => {
        const rowValues = columns.map(col => toSQLValue(row[col]));
        return `  (${rowValues.join(', ')})`;
      });

      sql += values.join(',\n');
      sql += '\nON CONFLICT DO NOTHING;\n\n';
    }

    return sql;
  } catch (err) {
    console.error(`   âŒ Error exporting ${tableName}:`, err.message);
    return `-- Error exporting ${tableName}: ${err.message}\n\n`;
  }
}

/**
 * ØªØµØ¯ÙŠØ± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
 */
async function exportAllData() {
  console.log('ğŸš€ Starting full database export...\n');

  const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, -5);
  const backupDir = path.join(__dirname, '..', 'database-backup');

  // Create backup directory
  if (!fs.existsSync(backupDir)) {
    fs.mkdirSync(backupDir, { recursive: true });
  }

  const dataFile = path.join(backupDir, `data-backup-${timestamp}.sql`);

  let fullSQL = `-- ============================================================================
-- DATABASE BACKUP - DATA ONLY
-- ============================================================================
-- Generated: ${new Date().toISOString()}
-- Source: ${supabaseUrl}
--
-- This file contains all data from the database.
-- To restore, first apply the schema (from migrations), then run this file.
-- ============================================================================

-- Disable triggers during import for better performance
SET session_replication_role = 'replica';

BEGIN;

`;

  // Export data from all tables
  for (const tableName of DATA_TABLES) {
    const tableSQL = await exportTableData(tableName);
    fullSQL += tableSQL;
  }

  fullSQL += `
COMMIT;

-- Re-enable triggers
SET session_replication_role = 'default';

-- Update sequences
DO $$
DECLARE
  seq_record RECORD;
  max_id BIGINT;
BEGIN
  FOR seq_record IN
    SELECT schemaname, sequencename, tablename
    FROM pg_sequences
    WHERE schemaname = 'public'
  LOOP
    EXECUTE format('SELECT COALESCE(MAX(id), 1) FROM %I.%I',
                   seq_record.schemaname, seq_record.tablename)
    INTO max_id;

    EXECUTE format('SELECT setval(%L, %s)',
                   seq_record.schemaname || '.' || seq_record.sequencename,
                   max_id);
  END LOOP;
END $$;

-- Verification query
SELECT
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
LIMIT 20;

-- ============================================================================
-- BACKUP COMPLETE
-- ============================================================================
`;

  // Write to file
  fs.writeFileSync(dataFile, fullSQL, 'utf8');

  console.log('\nâœ… Export completed successfully!');
  console.log(`ğŸ“ Data backup saved to: ${dataFile}`);
  console.log(`ğŸ“Š File size: ${(fs.statSync(dataFile).size / 1024 / 1024).toFixed(2)} MB`);

  return dataFile;
}

/**
 * Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©
 */
function createRestoreScript(backupDir) {
  const restoreScript = `#!/bin/bash

# ============================================================================
# Database Restore Script
# ============================================================================
# This script restores the database backup to a PostgreSQL server
#
# Usage:
#   1. Make executable: chmod +x restore-database.sh
#   2. Run: ./restore-database.sh
#
# Prerequisites:
#   - PostgreSQL client (psql) installed
#   - Database connection details in .env file
# ============================================================================

set -e

echo "ğŸ”„ Starting database restore..."

# Load environment variables
if [ -f .env ]; then
  export $(cat .env | grep -v '^#' | xargs)
else
  echo "âŒ Error: .env file not found"
  exit 1
fi

# Database connection details
DB_HOST=\${DB_HOST:-localhost}
DB_PORT=\${DB_PORT:-5432}
DB_NAME=\${DB_NAME:-consulate}
DB_USER=\${DB_USER:-postgres}

echo "ğŸ“Š Database: \${DB_NAME}"
echo "ğŸ–¥ï¸  Host: \${DB_HOST}:\${DB_PORT}"
echo "ğŸ‘¤ User: \${DB_USER}"
echo ""

# Get the latest backup file
SCHEMA_FILE="../../postgresql_schema/COMPLETE_SCHEMA.sql"
DATA_FILE=\$(ls -t data-backup-*.sql 2>/dev/null | head -1)

if [ ! -f "\$SCHEMA_FILE" ]; then
  echo "âŒ Error: Schema file not found: \$SCHEMA_FILE"
  exit 1
fi

if [ ! -f "\$DATA_FILE" ]; then
  echo "âŒ Error: No data backup file found"
  exit 1
fi

echo "ğŸ“„ Schema: \$SCHEMA_FILE"
echo "ğŸ“„ Data: \$DATA_FILE"
echo ""

read -p "âš ï¸  This will replace all data in \${DB_NAME}. Continue? (yes/no): " confirm
if [ "\$confirm" != "yes" ]; then
  echo "âŒ Restore cancelled"
  exit 1
fi

echo ""
echo "ğŸ”¨ Step 1/2: Applying schema..."
PGPASSWORD=\${DB_PASSWORD} psql -h \${DB_HOST} -p \${DB_PORT} -U \${DB_USER} -d \${DB_NAME} -f "\$SCHEMA_FILE"

echo ""
echo "ğŸ“¥ Step 2/2: Importing data..."
PGPASSWORD=\${DB_PASSWORD} psql -h \${DB_HOST} -p \${DB_PORT} -U \${DB_USER} -d \${DB_NAME} -f "\$DATA_FILE"

echo ""
echo "âœ… Database restore completed successfully!"
echo "ğŸ‰ Your database is now ready to use"
`;

  const restoreScriptPath = path.join(backupDir, 'restore-database.sh');
  fs.writeFileSync(restoreScriptPath, restoreScript, 'utf8');
  fs.chmodSync(restoreScriptPath, '755');

  console.log(`ğŸ“ Restore script created: ${restoreScriptPath}`);
}

/**
 * Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù README Ù„Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª
 */
function createReadme(backupDir) {
  const readme = `# Database Backup - Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©

## ğŸ“¦ Ù…Ø­ØªÙˆÙŠØ§Øª Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©

Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ù„Ø¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙƒØ§Ù…Ù„Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:

1. **Schema** (Ø§Ù„Ø¨Ù†ÙŠØ©): Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ \`postgresql_schema/COMPLETE_SCHEMA.sql\`
2. **Data** (Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª): Ù…Ù„Ù \`data-backup-*.sql\` ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ù„Ø¯
3. **Restore Script** (Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©): \`restore-database.sh\`

## ğŸš€ ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ±ÙØ±

### Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ (Ù…ÙˆØµÙ‰ Ø¨Ù‡)

\`\`\`bash
# 1. Ø§Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙƒØ§Ø¨
cd database-backup

# 2. ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„Ù .env ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰:
# DB_HOST=localhost
# DB_PORT=5432
# DB_NAME=consulate
# DB_USER=postgres
# DB_PASSWORD=your_password

# 3. Ø´ØºÙ„ Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©
./restore-database.sh
\`\`\`

### Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: ÙŠØ¯ÙˆÙŠØ§Ù‹

\`\`\`bash
# 1. Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø¨Ù†ÙŠØ© (Schema)
psql -h localhost -U postgres -d consulate -f ../postgresql_schema/COMPLETE_SCHEMA.sql

# 2. Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Data)
psql -h localhost -U postgres -d consulate -f data-backup-*.sql
\`\`\`

## ğŸ“‹ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª

- PostgreSQL 14 Ø£Ùˆ Ø£Ø­Ø¯Ø«
- psql (PostgreSQL client)
- Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙØ§Ø±ØºØ© Ø¬Ø§Ù‡Ø²Ø©

## âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ù‡Ù…Ø©

1. **Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰**:
   - Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
   - Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª ÙˆØ§Ù„Ø³ÙŠØ§Ø³Ø§Øª (RLS)
   - Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù€ Functions ÙˆØ§Ù„Ù€ Triggers
   - Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù€ Indexes

2. **Ù„Ø§ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰**:
   - Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø© (Storage)
   - Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Supabase Auth
   - Edge Functions

3. **Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©**:
   \`\`\`bash
   npm run backup-db
   # Ø£Ùˆ
   node scripts/export-full-database.js
   \`\`\`

## ğŸ” Ø§Ù„Ø£Ù…Ø§Ù†

- Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙÙŠ Ù…ÙƒØ§Ù† Ø¢Ù…Ù†
- Ù„Ø§ ØªØ´Ø§Ø±Ùƒ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ÙƒØ§Ø¨ Ø¹Ù„Ù†Ø§Ù‹ (ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø§Ø³Ø©)
- Ø§Ø³ØªØ®Ø¯Ù… ÙƒÙ„Ù…Ø§Øª Ù…Ø±ÙˆØ± Ù‚ÙˆÙŠØ© Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

## ğŸ“ Ø§Ù„Ø¯Ø¹Ù…

Ø¥Ø°Ø§ ÙˆØ§Ø¬Ù‡Øª Ø£ÙŠ Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©:
1. ØªØ­Ù‚Ù‚ Ù…Ù† Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
2. ØªØ­Ù‚Ù‚ Ù…Ù† ØµÙ„Ø§Ø­ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
3. Ø±Ø§Ø¬Ø¹ Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ (error logs)

---

ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙÙŠ: ${new Date().toISOString()}
`;

  const readmePath = path.join(backupDir, 'README.md');
  fs.writeFileSync(readmePath, readme, 'utf8');

  console.log(`ğŸ“– README created: ${readmePath}`);
}

// Run export
(async () => {
  try {
    const backupDir = path.join(__dirname, '..', 'database-backup');

    // Export data
    await exportAllData();

    // Create helper scripts
    createRestoreScript(backupDir);
    createReadme(backupDir);

    console.log('\n' + '='.repeat(80));
    console.log('âœ… BACKUP COMPLETED SUCCESSFULLY!');
    console.log('='.repeat(80));
    console.log('\nğŸ“ Backup location:', backupDir);
    console.log('\nğŸ“š Next steps:');
    console.log('   1. Copy the "database-backup" folder to your VPS');
    console.log('   2. Copy the "postgresql_schema" folder to your VPS');
    console.log('   3. Run: cd database-backup && ./restore-database.sh');
    console.log('\nğŸ’¡ For detailed instructions, see: database-backup/README.md\n');
  } catch (error) {
    console.error('\nâŒ Error during export:', error.message);
    console.error(error.stack);
    process.exit(1);
  }
})();
